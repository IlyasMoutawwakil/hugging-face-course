{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c3c442c",
   "metadata": {},
   "source": [
    "## Handling multiple sequences\n",
    "In the previous section, we explored the simplest of use cases: doing inference on a single sequence of a small length. However, some questions emerge already:\n",
    "\n",
    "- How do we handle multiple sequences?\n",
    "- How do we handle multiple sequences of different lengths?\n",
    "- Are vocabulary indices the only inputs that allow a model to work well?\n",
    "- Is there such a thing as too long a sequence?\n",
    "\n",
    "Letâ€™s see what kinds of problems these questions pose, and how we can solve them using the ðŸ¤— Transformers API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3c0834",
   "metadata": {},
   "source": [
    "### Models expect a batch of inputs\n",
    "In the previous exercise you saw how sequences get translated into lists of numbers. Letâ€™s convert this list of numbers to a tensor and send it to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71214100",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TFSequenceClassifierOutput(loss=None, logits=<tf.Tensor: shape=(1, 2), dtype=float32, numpy=array([[-2.7276196,  2.8789365]], dtype=float32)>, hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "input_ids = tf.constant(ids)\n",
    "\n",
    "# This line will fail.\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313644ce",
   "metadata": {},
   "source": [
    "Oh no! Why did this fail? â€œWe followed the steps from the pipeline in section 2.\n",
    "\n",
    "The problem is that we sent a single sequence to the model, whereas ðŸ¤— Transformers models expect multiple sentences by default. Here we tried to do everything the tokenizer did behind the scenes when we applied it to a sequence, but if you look closely, youâ€™ll see that it didnâ€™t just convert the list of input IDs into a tensor, it added a dimension on top of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93a91733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[  101  1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026\n",
      "   2878  2166  1012   102]], shape=(1, 16), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "tokenized_inputs = tokenizer(sequence, return_tensors=\"tf\")\n",
    "print(tokenized_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7ad197",
   "metadata": {},
   "source": [
    "Letâ€™s try again and add a new dimension:\n",
    "\n",
    "We will print the input IDs as well as the resulting logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554895d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input IDs: tf.Tensor(\n",
      "[[ 1045  1005  2310  2042  3403  2005  1037 17662 12172  2607  2026  2878\n",
      "   2166  1012]], shape=(1, 14), dtype=int32)\n",
      "Logits: tf.Tensor([[-2.7276196  2.8789365]], shape=(1, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "input_ids = tf.constant([ids])\n",
    "print(\"Input IDs:\", input_ids)\n",
    "\n",
    "output = model(input_ids)\n",
    "print(\"Logits:\", output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95932368",
   "metadata": {},
   "source": [
    "Batching is the act of sending multiple sentences through the model, all at once. If you only have one sentence, you can just build a batch with a single sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f947514",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [ids, ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681a099",
   "metadata": {},
   "source": [
    "This is a batch of two identical sequences!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43050025",
   "metadata": {},
   "source": [
    "Batching allows the model to work when you feed it multiple sentences. Using multiple sequences is just as simple as building a batch with a single sequence. <br>\n",
    "Thereâ€™s a second issue, though. When youâ€™re trying to batch together two (or more) sentences, they might be of different lengths. If youâ€™ve ever worked with tensors before, you know that **they need to be of rectangular shape**, so you wonâ€™t be able to convert the list of input IDs into a tensor directly. To work around this problem, we usually **pad the inputs.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb60286",
   "metadata": {},
   "source": [
    "### Padding the inputs\n",
    "The following list of lists cannot be converted to a tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7728ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1635b421",
   "metadata": {},
   "source": [
    "In order to work around this, **weâ€™ll use padding to make our tensors have a rectangular shape.** **Padding makes sure all our sentences have the same length by adding a special word called the padding token to the sentences with fewer values.** For example, if you have 10 sentences with 10 words and 1 sentence with 20 words, padding will ensure all the sentences have 20 words. In our example, the resulting tensor looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63bdf56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_id = 100\n",
    "\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, padding_id],\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d5a08",
   "metadata": {},
   "source": [
    "The padding token ID can be found in **tokenizer.pad_token_id**. Letâ€™s use it and send our two sentences through the model individually and batched together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eb90f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_59']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 1.5693668 -1.3894569]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor([[ 0.58029974 -0.4125235 ]], shape=(1, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 1.5693666 -1.3894575]\n",
      " [ 1.3373479 -1.2163188]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequence1_ids = [[200, 200, 200]]\n",
    "sequence2_ids = [[200, 200]]\n",
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "print(model(tf.constant(sequence1_ids)).logits)\n",
    "print(model(tf.constant(sequence2_ids)).logits)\n",
    "print(model(tf.constant(batched_ids)).logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ef0fa7",
   "metadata": {},
   "source": [
    "Thereâ€™s something wrong with the logits in our batched predictions: the second row should be the same as the logits for the second sentence, but weâ€™ve got completely different values!\n",
    "\n",
    "This is because the key feature of Transformer models is **attention layers** that contextualize each token. **These will take into account the padding tokens since they attend to all of the tokens of a sequence.** To get the same result when passing individual sentences of different lengths through the model or when passing a batch with the same sentences and padding applied, **we need to tell those attention layers to ignore the padding tokens.** This is done by using an **attention mask**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17ced91",
   "metadata": {},
   "source": [
    "### Attention masks\n",
    "**Attention masks are tensors with the exact same shape as the input IDs tensor, filled with 0s and 1s:** \n",
    "- **1s indicate the corresponding tokens should be attended to**, \n",
    "- **0s indicate the corresponding tokens should not be attended to (i.e., they should be ignored by the attention layers of the model).**\n",
    "\n",
    "Letâ€™s complete the previous example with an attention mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bdf5938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 1.5693666  -1.3894575 ]\n",
      " [ 0.5803004  -0.41252446]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "batched_ids = [\n",
    "    [200, 200, 200],\n",
    "    [200, 200, tokenizer.pad_token_id],\n",
    "]\n",
    "\n",
    "attention_mask = [\n",
    "    [1, 1, 1],\n",
    "    [1, 1, 0],\n",
    "]\n",
    "\n",
    "outputs = model(tf.constant(batched_ids), attention_mask=tf.constant(attention_mask))\n",
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71749df2",
   "metadata": {},
   "source": [
    "Now we get the same logits for the second sentence in the batch.\n",
    "\n",
    "Notice how the last value of the second sequence is a padding ID, which is a 0 value in the attention mask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79549440",
   "metadata": {},
   "source": [
    "### Longer sequences\n",
    "With Transformer models, there is a limit to the lengths of the sequences we can pass the models. **Most models handle sequences of up to 512 or 1024 tokens, and will crash when asked to process longer sequences.** There are two solutions to this problem:\n",
    "- Use a model with a longer supported sequence length.\n",
    "- Truncate your sequences.\n",
    "\n",
    "Models have different supported sequence lengths, and some specialize in handling very long sequences. [Longformer](https://huggingface.co/docs/transformers/model_doc/longformer) is one example, and another is [LED](https://huggingface.co/docs/transformers/model_doc/led). If youâ€™re working on a task that requires very long sequences, we recommend you take a look at those models.\n",
    "\n",
    "Otherwise, we recommend you truncate your sequences by specifying the **max_sequence_length** parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf5787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = sequence[:max_sequence_length]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
