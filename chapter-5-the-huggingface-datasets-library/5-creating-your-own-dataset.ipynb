{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IlyasMoutawwakil/huggingface-course/blob/main/chapter-5-the-huggingface-datasets-library/5-creating-your-own-dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFPjRHHo6eBJ"
      },
      "source": [
        "### Creating your own dataset\n",
        "Sometimes the dataset that you need to build an NLP application doesn‚Äôt exist, so you‚Äôll need to create it yourself. In this section we‚Äôll show you how to create a corpus of GitHub issues, which are commonly used to track bugs or features in GitHub repositories. This corpus could be used for various purposes, including:\n",
        "\n",
        "- Exploring how long it takes to close open issues or pull requests\n",
        "- Training a multilabel classifier that can tag issues with metadata based on the issue‚Äôs description (e.g., ‚Äúbug,‚Äù ‚Äúenhancement,‚Äù or ‚Äúquestion‚Äù)\n",
        "- Creating a semantic search engine to find which issues match a user‚Äôs query\n",
        "\n",
        "Here we‚Äôll focus on creating the corpus, and in the next section we‚Äôll tackle the semantic search application. To keep things meta, we‚Äôll use the GitHub issues associated with a popular open source project: ü§ó Datasets! Let‚Äôs take a look at how to get the data and explore the information contained in these issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W22xrg_6eBS"
      },
      "source": [
        "First, You will need to setup git, adapt your email and name. (in a code cell)\n",
        "\n",
        "!git config --global user.email \"you@example.com\"\n",
        "\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXT-Scxp6eBT"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "935f9bc3818848659210bfe16d54c18d"
          ]
        },
        "id": "xi6SpX_86eBU",
        "outputId": "dacdac37-e689-465f-d6dd-a17c9bca4fe6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "935f9bc3818848659210bfe16d54c18d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uswEpSDd6eBX"
      },
      "source": [
        "### Getting the data\n",
        "You can find all the issues in ü§ó Datasets by navigating to the repository‚Äôs [Issues tab](https://github.com/huggingface/datasets/issues). As shown in the following screenshot, at the time of writing there were 425 open issues and 812 closed ones.\n",
        "\n",
        "If you click on one of these issues you‚Äôll find it **contains a title, a description, and a set of labels** that characterize the issue. An example is shown in the screenshot below.\n",
        "![](https://huggingface.co/course/static/chapter5/datasets-issues-single.png \"Issues\")\n",
        "\n",
        "To download all the repository‚Äôs issues, we‚Äôll use the [GitHub REST API](https://docs.github.com/en/rest) to poll the [Issues endpoint](https://docs.github.com/en/rest/reference/issues#list-repository-issues). This endpoint returns a list of JSON objects, with each object containing a large number of fields that include the title and description as well as metadata about the status of the issue and so on.\n",
        "\n",
        "A convenient way to download the issues is via the **requests** library, which is the standard way for making HTTP requests in Python. You can install the library by running:\n",
        "\n",
        "$ !pip install requests\n",
        "\n",
        "Once the library is installed, you can make GET requests to the **Issues endpoint** by invoking the **requests.get()** function. For example, you can run the following command to retrieve the first issue on the first page:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWtMJTrf6eBY"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "url = \"https://api.github.com/repos/huggingface/datasets/issues?page=1&per_page=1\"\n",
        "response = requests.get(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbWFup7f6eBY"
      },
      "source": [
        "The response object contains a lot of useful information about the request, including the HTTP status code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3xrpum_6eBZ",
        "outputId": "a16be168-c212-4737-965d-14ca57a1bc88"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.status_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGWBa6n06eBa"
      },
      "source": [
        "where a 200 status means the request was successful (you can find a list of possible HTTP status codes [here](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)). What we are really interested in, though, is the **payload**, which can be accessed in various formats like bytes, strings, or JSON. Since we know our issues are in JSON format, let‚Äôs inspect the payload as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fRSQ4Yg6eBb",
        "outputId": "c044d588-0ede-424c-b3d1-3a7ca318f2d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/3618',\n",
              "  'repository_url': 'https://api.github.com/repos/huggingface/datasets',\n",
              "  'labels_url': 'https://api.github.com/repos/huggingface/datasets/issues/3618/labels{/name}',\n",
              "  'comments_url': 'https://api.github.com/repos/huggingface/datasets/issues/3618/comments',\n",
              "  'events_url': 'https://api.github.com/repos/huggingface/datasets/issues/3618/events',\n",
              "  'html_url': 'https://github.com/huggingface/datasets/issues/3618',\n",
              "  'id': 1112123365,\n",
              "  'node_id': 'I_kwDODunzps5CSafl',\n",
              "  'number': 3618,\n",
              "  'title': 'TIMIT Dataset not working with GPU',\n",
              "  'user': {'login': 'TheSeamau5',\n",
              "   'id': 3227869,\n",
              "   'node_id': 'MDQ6VXNlcjMyMjc4Njk=',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/3227869?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/TheSeamau5',\n",
              "   'html_url': 'https://github.com/TheSeamau5',\n",
              "   'followers_url': 'https://api.github.com/users/TheSeamau5/followers',\n",
              "   'following_url': 'https://api.github.com/users/TheSeamau5/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/TheSeamau5/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/TheSeamau5/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/TheSeamau5/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/TheSeamau5/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/TheSeamau5/repos',\n",
              "   'events_url': 'https://api.github.com/users/TheSeamau5/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/TheSeamau5/received_events',\n",
              "   'type': 'User',\n",
              "   'site_admin': False},\n",
              "  'labels': [{'id': 1935892857,\n",
              "    'node_id': 'MDU6TGFiZWwxOTM1ODkyODU3',\n",
              "    'url': 'https://api.github.com/repos/huggingface/datasets/labels/bug',\n",
              "    'name': 'bug',\n",
              "    'color': 'd73a4a',\n",
              "    'default': True,\n",
              "    'description': \"Something isn't working\"}],\n",
              "  'state': 'open',\n",
              "  'locked': False,\n",
              "  'assignee': None,\n",
              "  'assignees': [],\n",
              "  'milestone': None,\n",
              "  'comments': 0,\n",
              "  'created_at': '2022-01-24T03:26:03Z',\n",
              "  'updated_at': '2022-01-24T03:26:03Z',\n",
              "  'closed_at': None,\n",
              "  'author_association': 'NONE',\n",
              "  'active_lock_reason': None,\n",
              "  'body': '## Describe the bug\\r\\nI am working trying to use the TIMIT dataset in order to fine-tune Wav2Vec2 model and I am unable to load the \"audio\" column from the dataset when working with a GPU. \\r\\n\\r\\nI am working on Amazon Sagemaker Studio, on the Python 3 (PyTorch 1.8 Python 3.6 GPU Optimized) environment, with a single ml.g4dn.xlarge instance (corresponds to a Tesla T4 GPU). \\r\\n\\r\\nI don\\'t know if the issue is GPU related or Python environment related because everything works when I work off of the CPU Optimized environment with a non-GPU instance. My code also works on Google Colab with a GPU instance. \\r\\n\\r\\nThis issue is blocking because I cannot get the \\'audio\\' column in any way due to this error, which means that I can\\'t pass it to any functions. I later use the dataset.map function and that is where I originally noticed this error. \\r\\n\\r\\n## Steps to reproduce the bug\\r\\n```python\\r\\nfrom datasets import load_dataset\\r\\n\\r\\ntimit_train = load_dataset(\\'timit_asr\\', split=\\'train\\')\\r\\nprint(timit_train[\\'audio\\'])\\r\\n```\\r\\n\\r\\n## Expected results\\r\\nExpected to see inside the \\'audio\\' column, which contains an \\'array\\' nested field with the array data I actually need.\\r\\n\\r\\n## Actual results\\r\\n\\r\\nTraceback\\r\\n```\\r\\n---------------------------------------------------------------------------\\r\\nTypeError                                 Traceback (most recent call last)\\r\\n<ipython-input-6-ceeac555e921> in <module>\\r\\n----> 1 timit_train[\\'audio\\']\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py in __getitem__(self, key)\\r\\n   1917         \"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\\r\\n   1918         return self._getitem(\\r\\n-> 1919             key,\\r\\n   1920         )\\r\\n   1921 \\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/arrow_dataset.py in _getitem(self, key, decoded, **kwargs)\\r\\n   1902         pa_subtable = query_table(self._data, key, indices=self._indices if self._indices is not None else None)\\r\\n   1903         formatted_output = format_table(\\r\\n-> 1904             pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\\r\\n   1905         )\\r\\n   1906         return formatted_output\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/formatting/formatting.py in format_table(table, key, formatter, format_columns, output_all_columns)\\r\\n    529     python_formatter = PythonFormatter(features=None)\\r\\n    530     if format_columns is None:\\r\\n--> 531         return formatter(pa_table, query_type=query_type)\\r\\n    532     elif query_type == \"column\":\\r\\n    533         if key in format_columns:\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/formatting/formatting.py in __call__(self, pa_table, query_type)\\r\\n    280             return self.format_row(pa_table)\\r\\n    281         elif query_type == \"column\":\\r\\n--> 282             return self.format_column(pa_table)\\r\\n    283         elif query_type == \"batch\":\\r\\n    284             return self.format_batch(pa_table)\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/formatting/formatting.py in format_column(self, pa_table)\\r\\n    315         column = self.python_arrow_extractor().extract_column(pa_table)\\r\\n    316         if self.decoded:\\r\\n--> 317             column = self.python_features_decoder.decode_column(column, pa_table.column_names[0])\\r\\n    318         return column\\r\\n    319 \\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/formatting/formatting.py in decode_column(self, column, column_name)\\r\\n    221 \\r\\n    222     def decode_column(self, column: list, column_name: str) -> list:\\r\\n--> 223         return self.features.decode_column(column, column_name) if self.features else column\\r\\n    224 \\r\\n    225     def decode_batch(self, batch: dict) -> dict:\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/features/features.py in decode_column(self, column, column_name)\\r\\n   1337         return (\\r\\n   1338             [self[column_name].decode_example(value) if value is not None else None for value in column]\\r\\n-> 1339             if self._column_requires_decoding[column_name]\\r\\n   1340             else column\\r\\n   1341         )\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/features/features.py in <listcomp>(.0)\\r\\n   1336         \"\"\"\\r\\n   1337         return (\\r\\n-> 1338             [self[column_name].decode_example(value) if value is not None else None for value in column]\\r\\n   1339             if self._column_requires_decoding[column_name]\\r\\n   1340             else column\\r\\n\\r\\n/opt/conda/lib/python3.6/site-packages/datasets/features/audio.py in decode_example(self, value)\\r\\n     85             dict\\r\\n     86         \"\"\"\\r\\n---> 87         path, file = (value[\"path\"], BytesIO(value[\"bytes\"])) if value[\"bytes\"] is not None else (value[\"path\"], None)\\r\\n     88         if path is None and file is None:\\r\\n     89             raise ValueError(f\"An audio sample should have one of \\'path\\' or \\'bytes\\' but both are None in {value}.\")\\r\\n\\r\\nTypeError: string indices must be integers\\r\\n```\\r\\n\\r\\n## Environment info\\r\\n<!-- You can run the command `datasets-cli env` and copy-and-paste its output below. -->\\r\\n- `datasets` version: 1.18.0\\r\\n- Platform: Linux-4.14.256-197.484.amzn2.x86_64-x86_64-with-debian-buster-sid\\r\\n- Python version: 3.6.13\\r\\n- PyArrow version: 6.0.1\\r\\n',\n",
              "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/3618/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'timeline_url': 'https://api.github.com/repos/huggingface/datasets/issues/3618/timeline',\n",
              "  'performed_via_github_app': None}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd-bgLfj6eBc"
      },
      "source": [
        "Whoa, that‚Äôs a lot of information! We can see useful fields like title, body, and number that describe the issue, as well as information about the GitHub user who opened the issue."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTsaHWqQ6eBd"
      },
      "source": [
        "As described in the GitHub [documentation](https://docs.github.com/en/rest/overview/resources-in-the-rest-api#rate-limiting), unauthenticated requests are limited to 60 requests per hour. Although you can increase the per_page query parameter to reduce the number of requests you make, you will still hit the rate limit on any repository that has more than a few thousand issues. So instead, you should follow GitHub‚Äôs [instructions](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) on creating a personal access token so that you can boost the rate limit to 5,000 requests per hour. Once you have your token, you can include it as part of the request header:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfsWBW906eBe"
      },
      "source": [
        "```\n",
        "GITHUB_TOKEN = \"xxx\"  # Copy your GitHub token here\n",
        "headers = {\"Authorization\": f\"token {GITHUB_TOKEN}\"}\n",
        "```\n",
        "\n",
        "‚ö†Ô∏è Do not share a notebook with your GITHUB_TOKEN pasted in it. We recommend you delete the last cell once you have executed it to avoid leaking this information accidentally. Even better, store the token in a .env file and use the [python-dotenv library](https://github.com/theskumar/python-dotenv) to load it automatically for you as an environment variable.\n",
        "\n",
        "Now that we have our access token, let‚Äôs create a function that can download all the issues from a GitHub repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23vYxTP26eBf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import math\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VoTps3I6eBf"
      },
      "outputs": [],
      "source": [
        "def fetch_issues(\n",
        "    owner=\"huggingface\",\n",
        "    repo=\"datasets\",\n",
        "    num_issues=10_000,\n",
        "    rate_limit=5_000,\n",
        "    issues_path=Path(\".\"),\n",
        "):\n",
        "    if not issues_path.is_dir():\n",
        "        issues_path.mkdir(exist_ok=True)\n",
        "    \n",
        "    batch = []\n",
        "    all_issues = []\n",
        "    per_page = 100 # Number of issues to return per page\n",
        "    num_pages = math.ceil(num_issues / per_page)\n",
        "    base_url = \"https://api.github.com/repos\"\n",
        "\n",
        "    for page in tqdm(range(num_pages)):\n",
        "        # Query with state=all to get both open and closed issues\n",
        "        query = f\"issues?page={page}&per_page={per_page}&state=all\"\n",
        "        issues = requests.get(f\"{base_url}/{owner}/{repo}/{query}\", headers=headers)\n",
        "        batch.extend(issues.json())\n",
        "\n",
        "        if len(batch) > rate_limit and len(all_issues) < num_issues:\n",
        "            all_issues.extend(batch)\n",
        "            batch = []  # Flush batch for next time period\n",
        "            print(f\"Reached GitHub rate limit. Sleeping for one hour ...\")\n",
        "            time.sleep(60 * 60 + 1)\n",
        "\n",
        "    all_issues.extend(batch)\n",
        "    df = pd.DataFrame.from_records(all_issues)\n",
        "    df.to_json(f\"{issues_path}/{repo}-issues.jsonl\", orient=\"records\", lines=True)\n",
        "    print(\n",
        "        f\"Downloaded all the issues for {repo}! Dataset stored at {issues_path}/{repo}-issues.jsonl\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpkQA0CT6eBg"
      },
      "source": [
        "Now when we call fetch_issues() it will download all the issues in batches to avoid exceeding GitHub‚Äôs limit on the number of requests per hour; the result will be stored in a repository_name-issues.jsonl file, where each line is a JSON object the represents an issue. Let‚Äôs use this function to grab all the issues from ü§ó Datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "9d51bae4869e4aa996632df78a443b3e"
          ]
        },
        "id": "cr_FjDxB6eBg",
        "outputId": "ceb06d3d-d910-4e6c-99a6-bc532bd29bc7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d51bae4869e4aa996632df78a443b3e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/100 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloaded all the issues for datasets! Dataset stored at ./datasets-issues.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Depending on your internet connection, this can take several minutes to run...\n",
        "fetch_issues()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmTcznl76eBh"
      },
      "source": [
        "Once the issues are downloaded we can load them locally using our newfound skills from [section 2](https://huggingface.co/course/chapter5/2?fw=tf):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiSYL96X6eBh",
        "outputId": "b19c2d98-509f-4de9-a3bb-21b50b0dab77"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using custom data configuration default-53a97158fb81485c\n",
            "Reusing dataset json (C:\\Users\\batuh\\.cache\\huggingface\\datasets\\json\\default-53a97158fb81485c\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['url', 'repository_url', 'labels_url', 'comments_url', 'events_url', 'html_url', 'id', 'node_id', 'number', 'title', 'user', 'labels', 'state', 'locked', 'assignee', 'assignees', 'milestone', 'comments', 'created_at', 'updated_at', 'closed_at', 'author_association', 'active_lock_reason', 'body', 'reactions', 'timeline_url', 'performed_via_github_app', 'draft', 'pull_request'],\n",
              "    num_rows: 3678\n",
              "})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "issues_dataset = load_dataset(\"json\", data_files=\"datasets-issues.jsonl\", split=\"train\")\n",
        "issues_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHFCvVjh6eBh"
      },
      "source": [
        "Great, we‚Äôve created our first dataset from scratch! But why are there several thousand issues when the [Issues](https://github.com/huggingface/datasets/issues) tab of the ü§ó Datasets repository only shows around 1,000 issues in total ü§î? As described in the GitHub [documentation](https://docs.github.com/en/rest/reference/issues#list-issues-assigned-to-the-authenticated-user), that‚Äôs because we‚Äôve downloaded all the pull requests as well:\n",
        "\n",
        "> GitHub‚Äôs REST API v3 considers every pull request an issue, but not every issue is a pull request. For this reason, ‚ÄúIssues‚Äù endpoints may return both issues and pull requests in the response. You can identify pull requests by the **pull_request** key. Be aware that **the id of a pull request returned from ‚ÄúIssues‚Äù endpoints will be an issue id.**\n",
        "\n",
        "Since the contents of issues and pull requests are quite different, let‚Äôs do some minor preprocessing to enable us to distinguish between them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZh9wYt56eBi"
      },
      "source": [
        "### Cleaning up the data\n",
        "The above snippet from GitHub‚Äôs documentation tells us that the pull_request column can be used to differentiate between issues and pull requests. Let‚Äôs look at a random sample to see what the difference is. As we did in [section 3](https://huggingface.co/course/chapter5/3), we‚Äôll chain **Dataset.shuffle()** and **Dataset.select()** to create a random sample and then zip the html_url and pull_request columns so we can compare the various URLs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQ0OnrO96eBi",
        "outputId": "7c4e0f5c-197f-4b0e-fa1c-bdaaba0fe2f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading cached shuffled indices for dataset at C:\\Users\\batuh\\.cache\\huggingface\\datasets\\json\\default-53a97158fb81485c\\0.0.0\\c90812beea906fcffe0d5e3bb9eba909a80a998b5f88e9f8acbd320aa91acfde\\cache-6cfa73461b25fa32.arrow\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> URL: https://github.com/huggingface/datasets/pull/1126\n",
            ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/1126', 'html_url': 'https://github.com/huggingface/datasets/pull/1126', 'diff_url': 'https://github.com/huggingface/datasets/pull/1126.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/1126.patch', 'merged_at': None}\n",
            "\n",
            ">> URL: https://github.com/huggingface/datasets/pull/892\n",
            ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/892', 'html_url': 'https://github.com/huggingface/datasets/pull/892', 'diff_url': 'https://github.com/huggingface/datasets/pull/892.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/892.patch', 'merged_at': datetime.datetime(2020, 11, 27, 18, 8, 44)}\n",
            "\n",
            ">> URL: https://github.com/huggingface/datasets/pull/2950\n",
            ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/2950', 'html_url': 'https://github.com/huggingface/datasets/pull/2950', 'diff_url': 'https://github.com/huggingface/datasets/pull/2950.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/2950.patch', 'merged_at': datetime.datetime(2021, 9, 20, 15, 28, 1)}\n",
            "\n",
            ">> URL: https://github.com/huggingface/datasets/pull/3275\n",
            ">> Pull request: {'url': 'https://api.github.com/repos/huggingface/datasets/pulls/3275', 'html_url': 'https://github.com/huggingface/datasets/pull/3275', 'diff_url': 'https://github.com/huggingface/datasets/pull/3275.diff', 'patch_url': 'https://github.com/huggingface/datasets/pull/3275.patch', 'merged_at': datetime.datetime(2021, 11, 15, 14, 45, 23)}\n",
            "\n",
            ">> URL: https://github.com/huggingface/datasets/issues/1634\n",
            ">> Pull request: None\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sample = issues_dataset.shuffle(seed=42).select(range(5))\n",
        "\n",
        "# Print out the URL and pull request entries\n",
        "for url, pr in zip(sample[\"html_url\"], sample[\"pull_request\"]):\n",
        "    print(f\">> URL: {url}\")\n",
        "    print(f\">> Pull request: {pr}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScBAyiE16eBj"
      },
      "source": [
        "Here we can see that each pull request is associated with various URLs, while ordinary issues have a None entry. **We can use this distinction to create a new is_pull_request column that checks whether the pull_request field is None or not:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNSUcdXl6eBj",
        "outputId": "1ed79f79-eb2a-4932-defc-317b81f27bf8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3678/3678 [00:01<00:00, 1989.47ex/s]\n"
          ]
        }
      ],
      "source": [
        "issues_dataset = issues_dataset.map(\n",
        "    lambda x: {\"is_pull_request\": False if x[\"pull_request\"] is None else True}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kV_zkzJI6eBk"
      },
      "source": [
        "Although we could proceed to further clean up the dataset by dropping or renaming some columns, it is generally a good practice to keep the dataset as ‚Äúraw‚Äù as possible at this stage so that it can be easily used in multiple applications.\n",
        "\n",
        "Before we push our dataset to the Hugging Face Hub, let‚Äôs deal with one thing that‚Äôs missing from it: the comments associated with each issue and pull request. We‚Äôll add them next with ‚Äî you guessed it ‚Äî the GitHub REST API!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5xadAo66eBk"
      },
      "source": [
        "### Augmenting the dataset\n",
        "As shown in the following screenshot, the comments associated with an issue or pull request provide a rich source of information, especially if we‚Äôre interested in building a search engine to answer user queries about the library.\n",
        "![](https://huggingface.co/course/static/chapter5/datasets-issues-comment.png \"datasets-issues-comment\")\n",
        "\n",
        "The GitHub REST API provides a [Comments endpoint](https://docs.github.com/en/rest/reference/issues#list-issue-comments) that returns all the comments associated with an issue number. Let‚Äôs test the endpoint to see what it returns:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_C90rtE6eBl",
        "outputId": "dd06a710-1bd6-47f6-b9c0-07562c86527b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128',\n",
              "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-897594128',\n",
              "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
              "  'id': 897594128,\n",
              "  'node_id': 'IC_kwDODunzps41gDMQ',\n",
              "  'user': {'login': 'bhavitvyamalik',\n",
              "   'id': 19718818,\n",
              "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
              "   'html_url': 'https://github.com/bhavitvyamalik',\n",
              "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
              "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
              "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
              "   'type': 'User',\n",
              "   'site_admin': False},\n",
              "  'created_at': '2021-08-12T12:21:52Z',\n",
              "  'updated_at': '2021-08-12T12:31:17Z',\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'body': \"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
              "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/897594128/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'performed_via_github_app': None},\n",
              " {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889',\n",
              "  'html_url': 'https://github.com/huggingface/datasets/pull/2792#issuecomment-898644889',\n",
              "  'issue_url': 'https://api.github.com/repos/huggingface/datasets/issues/2792',\n",
              "  'id': 898644889,\n",
              "  'node_id': 'IC_kwDODunzps41kDuZ',\n",
              "  'user': {'login': 'bhavitvyamalik',\n",
              "   'id': 19718818,\n",
              "   'node_id': 'MDQ6VXNlcjE5NzE4ODE4',\n",
              "   'avatar_url': 'https://avatars.githubusercontent.com/u/19718818?v=4',\n",
              "   'gravatar_id': '',\n",
              "   'url': 'https://api.github.com/users/bhavitvyamalik',\n",
              "   'html_url': 'https://github.com/bhavitvyamalik',\n",
              "   'followers_url': 'https://api.github.com/users/bhavitvyamalik/followers',\n",
              "   'following_url': 'https://api.github.com/users/bhavitvyamalik/following{/other_user}',\n",
              "   'gists_url': 'https://api.github.com/users/bhavitvyamalik/gists{/gist_id}',\n",
              "   'starred_url': 'https://api.github.com/users/bhavitvyamalik/starred{/owner}{/repo}',\n",
              "   'subscriptions_url': 'https://api.github.com/users/bhavitvyamalik/subscriptions',\n",
              "   'organizations_url': 'https://api.github.com/users/bhavitvyamalik/orgs',\n",
              "   'repos_url': 'https://api.github.com/users/bhavitvyamalik/repos',\n",
              "   'events_url': 'https://api.github.com/users/bhavitvyamalik/events{/privacy}',\n",
              "   'received_events_url': 'https://api.github.com/users/bhavitvyamalik/received_events',\n",
              "   'type': 'User',\n",
              "   'site_admin': False},\n",
              "  'created_at': '2021-08-13T18:28:27Z',\n",
              "  'updated_at': '2021-08-13T18:28:27Z',\n",
              "  'author_association': 'CONTRIBUTOR',\n",
              "  'body': 'Thanks for the help, @albertvillanova! All tests are passing now.',\n",
              "  'reactions': {'url': 'https://api.github.com/repos/huggingface/datasets/issues/comments/898644889/reactions',\n",
              "   'total_count': 0,\n",
              "   '+1': 0,\n",
              "   '-1': 0,\n",
              "   'laugh': 0,\n",
              "   'hooray': 0,\n",
              "   'confused': 0,\n",
              "   'heart': 0,\n",
              "   'rocket': 0,\n",
              "   'eyes': 0},\n",
              "  'performed_via_github_app': None}]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issue_number = 2792\n",
        "url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "response = requests.get(url, headers=headers)\n",
        "response.json()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QeYg2IA6eBl"
      },
      "source": [
        "We can see that the comment is stored in the body field, so let‚Äôs write a simple function that returns all the comments associated with an issue by picking out the body contents for each element in response.json():"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZiyswbWS6eBm",
        "outputId": "5a6c9b27-da66-4c35-e978-c69f5515b5c2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"@albertvillanova my tests are failing here:\\r\\n```\\r\\ndataset_name = 'gooaq'\\r\\n\\r\\n    def test_load_dataset(self, dataset_name):\\r\\n        configs = self.dataset_tester.load_all_configs(dataset_name, is_local=True)[:1]\\r\\n>       self.dataset_tester.check_load_dataset(dataset_name, configs, is_local=True, use_local_dummy_data=True)\\r\\n\\r\\ntests/test_dataset_common.py:234: \\r\\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \\r\\ntests/test_dataset_common.py:187: in check_load_dataset\\r\\n    self.parent.assertTrue(len(dataset[split]) > 0)\\r\\nE   AssertionError: False is not true\\r\\n```\\r\\nWhen I try loading dataset on local machine it works fine. Any suggestions on how can I avoid this error?\",\n",
              " 'Thanks for the help, @albertvillanova! All tests are passing now.']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_comments(issue_number):\n",
        "    url = f\"https://api.github.com/repos/huggingface/datasets/issues/{issue_number}/comments\"\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return [r[\"body\"] for r in response.json()]\n",
        "\n",
        "# Test our function works as expected\n",
        "get_comments(2792)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRtzA1eB6eBm"
      },
      "source": [
        "This looks good, so let‚Äôs use **Dataset.map() to add a new comments column to each issue in our dataset:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rNHuZY06eBn",
        "outputId": "962c197f-2a30-4d80-96f0-1ffcb3784ff1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3678/3678 [24:42<00:00,  2.48ex/s]\n"
          ]
        }
      ],
      "source": [
        "# Depending on your internet connection, this can take a few minutes...\n",
        "issues_with_comments_dataset = issues_dataset.map(\n",
        "    lambda x: {\"comments\": get_comments(x[\"number\"])}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4toiM6Qe6eBn"
      },
      "source": [
        "The final step is to save the augmented dataset alongside our raw data so we can push them both to the Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4wMY77Z6eBn",
        "outputId": "60bc7d84-afc3-444f-aa8a-92070d17fd69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.72ba/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "16078073"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "issues_with_comments_dataset.to_json(\"issues-datasets-with-comments.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrquGHn56eBo"
      },
      "source": [
        "### Uploading the dataset to the Hugging Face Hub\n",
        "Now that we have our augmented dataset, it‚Äôs time to push it to the Hub so we can share it with the community! To upload the dataset we‚Äôll use the [ü§ó Hub library](https://github.com/huggingface/huggingface_hub), which allows us to interact with the Hugging Face Hub through a Python API. ü§ó Hub comes preinstalled with ü§ó Transformers, so we can use it directly. For example, we can use the **list_datasets()** function to get information about all the public datasets currently hosted on the Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZrNo-Qa6eBo",
        "outputId": "a0888fc3-0d89-4891-a90d-4dc6cd9fd955"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of datasets on Hub: 2627\n",
            "Dataset Name: GEM/e2e_nlg, Tags: []\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import list_datasets\n",
        "\n",
        "all_datasets = list_datasets()\n",
        "print(f\"Number of datasets on Hub: {len(all_datasets)}\")\n",
        "print(all_datasets[156])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DsH8yb8k6eBo"
      },
      "source": [
        "We can see that there are currently nearly 2,700 datasets on the Hub, and the list_datasets() function also provides some basic metadata about each dataset repository.\n",
        "\n",
        "For our purposes, the first thing we need to do is create a new dataset repository on the Hub. To do that we need an authentication token, which can be obtained by first logging into the Hugging Face Hub with the notebook_login() function:\n",
        "\n",
        "If you‚Äôre running the code in a terminal, you can log in via the CLI instead:\n",
        "\n",
        "> huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Km5qQLH6eBp"
      },
      "source": [
        "Once we‚Äôve done this, we can create a new dataset repository with the **create_repo()** function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMDz5znZ6eBp",
        "outputId": "3c8a6203-030c-4daf-8c71-afdf7f1f243c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'https://huggingface.co/datasets/BatuhanYilmaz/github-issues'"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "repo_url = create_repo(name=\"github-issues\", repo_type=\"dataset\")\n",
        "repo_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gL7-uREX6eBp"
      },
      "source": [
        "In this example, we‚Äôve created an empty dataset repository called github-issues under your username.\n",
        "\n",
        "Next, let‚Äôs clone the repository from the Hub to our local machine and copy our dataset file into it. ü§ó Hub provides a handy **Repository** class that wraps many of the common Git commands, so to clone the remote repository we simply need to provide the URL and local path we wish to clone to:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BE11bu7B6eBp",
        "outputId": "1aff0382-ca71-4899-dd62-1155c1ce3805"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\batuh\\HuggingFace-Course\\Chapter05\\github-issues is already a clone of https://huggingface.co/datasets/BatuhanYilmaz/github-issues. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The system cannot find the file specified.\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "repo = Repository(local_dir=\"github-issues\", clone_from=repo_url)\n",
        "!cp datasets-issues-with-comments.jsonl github-issues/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xy0eTFKt6eBq"
      },
      "source": [
        "By default, various file extensions (such as .bin, .gz, and .zip) are tracked with Git LFS so that large files can be versioned within the same Git workflow. You can find a list of tracked file extensions inside the repository‚Äôs .gitattributes file. To include the JSON Lines format in the list, we can run the following command:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPXkYsPl6eBq"
      },
      "outputs": [],
      "source": [
        "repo.lfs_track(\"*.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqFbymjG6eBq"
      },
      "source": [
        "Then we can use **Repository.push_to_hub()** to push the dataset to the Hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEZt6g5j6eBq"
      },
      "outputs": [],
      "source": [
        "repo.push_to_hub()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EZZ4MHF6eBr"
      },
      "source": [
        "If we navigate to the URL contained in repo_url, we should now see that our dataset file has been uploaded.\n",
        "\n",
        "From here, anyone can download the dataset by simply providing load_dataset() with the repository ID as the path argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gqc--OPh6eBr"
      },
      "outputs": [],
      "source": [
        "remote_dataset = load_dataset(\"BatuhanYilmaz/github-issues\", split=\"train\")\n",
        "remote_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JNZP6WI6eBr"
      },
      "source": [
        "Cool, we‚Äôve pushed our dataset to the Hub and it‚Äôs available for others to use! There‚Äôs just one important thing left to do: adding a dataset card that explains how the corpus was created and provides other useful information for the community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmM0RfeP6eBr"
      },
      "source": [
        "üí° You can also upload a dataset to the Hugging Face Hub directly from the terminal by using huggingface-cli and a bit of Git magic. See the [ü§ó Datasets guide](https://huggingface.co/docs/datasets/share.html#add-a-community-dataset) for details on how to do this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S76flV6V6eBs"
      },
      "source": [
        "### Creating a dataset card\n",
        "Well-documented datasets are more likely to be useful to others (including your future self!), as they provide the context to enable users to decide whether the dataset is relevant to their task and to evaluate any potential biases in or risks associated with using the dataset.\n",
        "\n",
        "On the Hugging Face Hub, this information is stored in each dataset repository‚Äôs README.md file. There are two main steps you should take before creating this file:\n",
        "\n",
        "1. Use the [datasets-tagging application](https://huggingface.co/spaces/huggingface/datasets-tagging) to create metadata tags in YAML format. These tags are used for a variety of search features on the Hugging Face Hub and ensure your dataset can be easily found by members of the community. Since we have created a custom dataset here, you‚Äôll need to clone the datasets-tagging repository and run the application locally. Here‚Äôs what the interface looks like:\n",
        "\n",
        "![](https://huggingface.co/course/static/chapter5/datasets-tagger.png \"datasets-tagger\")\n",
        "\n",
        "2. Read the [ü§ó Datasets guide](https://github.com/huggingface/datasets/blob/master/templates/README_guide.md) on creating informative dataset cards and use it as a template.\n",
        "You can create the README.md file directly on the Hub, and you can find a template dataset card in the lewtun/github-issues dataset repository. A screenshot of the filled-out dataset card is shown below.\n",
        "\n",
        "![](https://huggingface.co/course/static/chapter5/dataset-card.png \"dataset-card\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDO4WsEi6eBs"
      },
      "source": [
        "That‚Äôs it! We‚Äôve seen in this section that creating a good dataset can be quite involved, but fortunately uploading it and sharing it with the community is not. In the next section we‚Äôll use our new dataset to create a semantic search engine with ü§ó Datasets that can match questions to the most relevant issues and comments."
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "orig_nbformat": 4,
    "colab": {
      "name": "5-creating-your-own-dataset.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}