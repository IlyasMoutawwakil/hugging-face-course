{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What to do when you get an error\n",
    "In this section we‚Äôll look at some common errors that can occur when you‚Äôre trying to generate predictions from your freshly tuned Transformer model. This will prepare you for [section 4](https://huggingface.co/course/chapter8/4?fw=tf), where we‚Äôll explore how to debug the training phase itself.\n",
    "\n",
    "We‚Äôve prepared a [template model repository] for this section, and if you want to run the code in this chapter you‚Äôll first need to copy the model to your account on the [Hugging Face Hub](https://huggingface.co). To do so, first log in by running either the following in a Jupyter notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ccb6184ad3d490690f285c47ec27d91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or the following in your favorite terminal:\n",
    "\n",
    "> huggingface-cli login\n",
    "\n",
    "This will prompt you to enter your username and password, and will save a token under ~/.cache/huggingface/. Once you‚Äôve logged in, you can copy the template repository with the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from huggingface_hub import Repository, snapshot_download, create_repo, get_full_repo_name\n",
    "\n",
    "\n",
    "def copy_repository_template():\n",
    "    # Clone the repo and extract the local path\n",
    "    template_repo_id = \"lewtun/distilbert-base-uncased-finetuned-squad-d5716d28\"\n",
    "    commit_hash = \"be3eaffc28669d7932492681cd5f3e8905e358b4\"\n",
    "    template_repo_dir = snapshot_download(template_repo_id, revision=commit_hash)\n",
    "    # Create an empty repo on the Hub\n",
    "    model_name = template_repo_id.split(\"/\")[1]\n",
    "    create_repo(model_name, exist_ok=True)\n",
    "    # Clone the empty repo\n",
    "    new_repo_id = get_full_repo_name(model_name)\n",
    "    new_repo_dir = model_name\n",
    "    repo = Repository(local_dir=new_repo_dir, clone_from=new_repo_id)\n",
    "    # Copy files\n",
    "    copy_tree(template_repo_dir, new_repo_dir)\n",
    "    # Push to Hub\n",
    "    repo.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when you call copy_repository_template(), it will create a copy of the template repository under your account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the pipeline from ü§ó Transformers\n",
    "To kick off our journey into the wonderful world of debugging Transformer models, consider the following scenario: you‚Äôre working with a colleague on a question answering project to help the customers of an e-commerce website find answers about consumer products. Your colleague shoots you a message like:\n",
    "> G‚Äôday! I just ran an experiment using the techniques in [Chapter 7](https://huggingface.co/course/chapter7/7) of the Hugging Face course and got some great results on SQuAD! I think we can use this model as a starting point for our project. The model ID on the Hub is ‚Äúlewtun/distillbert-base-uncased-finetuned-squad-d5716d28‚Äù. Feel free to test it out :)\n",
    "\n",
    "and the first thing you think of is to load the model using the pipeline from ü§ó Transformers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: https://huggingface.co/api/models/BatuhanYilmaz/distillbert-base-uncased-finetuned-squad-d5716d28",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m      3\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m get_full_repo_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistillbert-base-uncased-finetuned-squad-d5716d28\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\__init__.py:535\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(config, revision\u001b[38;5;241m=\u001b[39mrevision, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 535\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, revision\u001b[38;5;241m=\u001b[39mrevision, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    537\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:582\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    581\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 582\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:552\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    550\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 552\u001b[0m     configuration_file \u001b[38;5;241m=\u001b[39m \u001b[43mget_configuration_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    553\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    554\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    559\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(pretrained_model_name_or_path):\n\u001b[0;32m    560\u001b[0m         config_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pretrained_model_name_or_path, configuration_file)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:843\u001b[0m, in \u001b[0;36mget_configuration_file\u001b[1;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m    823\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    824\u001b[0m \u001b[38;5;124;03mGet the configuration file to use for this version of transformers.\u001b[39;00m\n\u001b[0;32m    825\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[38;5;124;03m    `str`: The configuration file to use.\u001b[39;00m\n\u001b[0;32m    841\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;66;03m# Inspect all files from the repo/folder.\u001b[39;00m\n\u001b[1;32m--> 843\u001b[0m all_files \u001b[38;5;241m=\u001b[39m \u001b[43mget_list_of_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    844\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\n\u001b[0;32m    845\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    846\u001b[0m configuration_files_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    847\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m all_files:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:2103\u001b[0m, in \u001b[0;36mget_list_of_files\u001b[1;34m(path_or_repo, revision, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2102\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlist_repo_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\hf_api.py:884\u001b[0m, in \u001b[0;36mHfApi.list_repo_files\u001b[1;34m(self, repo_id, revision, repo_type, token, timeout)\u001b[0m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;124;03mGet the list of files in a given repo.\u001b[39;00m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 884\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_info\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m repo_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    888\u001b[0m     info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_info(\n\u001b[0;32m    889\u001b[0m         repo_id, revision\u001b[38;5;241m=\u001b[39mrevision, token\u001b[38;5;241m=\u001b[39mtoken, timeout\u001b[38;5;241m=\u001b[39mtimeout\n\u001b[0;32m    890\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\hf_api.py:868\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[1;34m(self, repo_id, revision, token, timeout)\u001b[0m\n\u001b[0;32m    866\u001b[0m headers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauthorization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBearer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m} \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    867\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m--> 868\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    869\u001b[0m d \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m    870\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    957\u001b[0m     http_error_msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m Server Error: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m for url: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus_code, reason, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39murl)\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/api/models/BatuhanYilmaz/distillbert-base-uncased-finetuned-squad-d5716d28"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_checkpoint = get_full_repo_name(\"distillbert-base-uncased-finetuned-squad-d5716d28\")\n",
    "reader = pipeline(\"question-answering\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh no, something seems to have gone wrong! If you‚Äôre new to programming, these kind of errors can seem a bit cryptic at first (what even is an OSError?!). The error displayed here is just the last part of a much larger error report called a Python traceback (aka stack trace). \n",
    "\n",
    "There‚Äôs a lot of information contained in these reports, so let‚Äôs walk through the key parts together. The first thing to note is that **tracebacks should be read from bottom to top.** This might sound weird if you‚Äôre used to reading English text from top to bottom, but it reflects the fact that the traceback shows the sequence of function calls that the pipeline makes when downloading the model and tokenizer. (Check out [Chapter 2](https://huggingface.co/course/chapter2/1) for more details on how the pipeline works under the hood.)\n",
    "\n",
    "This means that the last line of the traceback indicates the last error message and gives the name of the exception that was raised. In this case, the exception type is OSError, which indicates a system-related error. If we read the accompanying error message, we can see that there seems to be a problem with the model‚Äôs config.json file, and we‚Äôre given two suggestions to fix it:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Make sure that:\n",
    "\n",
    "- 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'\n",
    "\n",
    "- or 'lewtun/distillbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üí° If you encounter an error message that is difficult to understand, just copy and paste the message into the Google or [Stack Overflow](https://stackoverflow.com) search bar (yes, really!). There‚Äôs a good chance that you‚Äôre not the first person to encounter the error, and this is a good way to find solutions that others in the community have posted. For example, searching for OSError: Can't load config for on Stack Overflow gives [several hits](https://stackoverflow.com/search?q=OSError%3A+Can%27t+load+config+for+&s=146957b5-e977-4fd6-8b12-6f84f655787e) that could be used as a starting point for solving the problem.\n",
    "\n",
    "The first suggestion is asking us to check whether the model ID is actually correct, so the first order of business is to copy the identifier and paste it into the Hub‚Äôs search bar:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/wrong-model-id.png)\n",
    "\n",
    "Hmm, it indeed looks like our colleague‚Äôs model is not on the Hub‚Ä¶ aha, but there‚Äôs a typo in the name of the model! DistilBERT only has one ‚Äúl‚Äù in its name, so let‚Äôs fix that and look for ‚Äúlewtun/distilbert-base-uncased-finetuned-squad-d5716d28‚Äù instead:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/true-model-id.png)\n",
    "\n",
    "Okay, this got a hit. Now let‚Äôs try to download the model again with the correct model ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "404 Client Error: Not Found for url: https://huggingface.co/BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28/resolve/main/config.json\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load config for 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:\n\n- 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is not a path to a local directory with something else, in that case)\n\n- or 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:568\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 568\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    574\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    575\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    576\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    577\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;66;03m# Load config dict\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:1776\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m   1775\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m-> 1776\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1777\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1778\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1782\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1783\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_auth_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_auth_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m   1787\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\file_utils.py:1948\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[0;32m   1947\u001b[0m r \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mhead(url, headers\u001b[38;5;241m=\u001b[39mheaders, allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout)\n\u001b[1;32m-> 1948\u001b[0m \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1949\u001b[0m etag \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX-Linked-Etag\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m r\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mETag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\requests\\models.py:960\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    959\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m--> 960\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28/resolve/main/config.json",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m model_checkpoint \u001b[38;5;241m=\u001b[39m get_full_repo_name(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-squad-d5716d28\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion-answering\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_checkpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\pipelines\\__init__.py:535\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    533\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(config, revision\u001b[38;5;241m=\u001b[39mrevision, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 535\u001b[0m     config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, revision\u001b[38;5;241m=\u001b[39mrevision, _from_pipeline\u001b[38;5;241m=\u001b[39mtask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[0;32m    537\u001b[0m model_name \u001b[38;5;241m=\u001b[39m model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# Infer the framework from the model\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# Forced if framework already defined, inferred if it's None\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# Will load the correct model if possible\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\configuration_auto.py:582\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    580\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_or_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m pretrained_model_name_or_path\n\u001b[0;32m    581\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 582\u001b[0m config_dict, _ \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m trust_remote_code:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\configuration_utils.py:593\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[1;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    591\u001b[0m         msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is a valid git identifier (branch name, a tag name, or a commit id) that exists for this model name as listed on its model page on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 593\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(msg)\n\u001b[0;32m    595\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (json\u001b[38;5;241m.\u001b[39mJSONDecodeError, \u001b[38;5;167;01mUnicodeDecodeError\u001b[39;00m):\n\u001b[0;32m    596\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    597\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt reach server at \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to download configuration file or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    598\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration file is not a valid JSON file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check network or file content here: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_config_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    600\u001b[0m     )\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load config for 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28'. Make sure that:\n\n- 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is a correct model identifier listed on 'https://huggingface.co/models'\n  (make sure 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is not a path to a local directory with something else, in that case)\n\n- or 'BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28' is the correct path to a directory containing a config.json file\n\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = get_full_repo_name(\"distilbert-base-uncased-finetuned-squad-d5716d28\")\n",
    "reader = pipeline(\"question-answering\", model=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Argh, foiled again ‚Äî welcome to the daily life of a machine learning engineer! Since we‚Äôve fixed the model ID, the problem must lie in the repository itself. A quick way to access the contents of a repository on the ü§ó Hub is via the **list_repo_files()** function of the huggingface_hub library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.gitattributes',\n",
       " 'README.md',\n",
       " 'pytorch_model.bin',\n",
       " 'special_tokens_map.json',\n",
       " 'tokenizer_config.json',\n",
       " 'training_args.bin',\n",
       " 'vocab.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "\n",
    "list_repo_files(repo_id=model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting ‚Äî **there doesn‚Äôt seem to be a config.json file in the repository!** No wonder our pipeline couldn‚Äôt load the model; our colleague must have forgotten to push this file to the Hub after they fine-tuned it. In this case, the problem seems pretty straightforward to fix: we could ask them to add the file, or, since we can see from the model ID that the pretrained model used was [distilbert-base-uncased](https://huggingface.co/distilbert-base-uncased), we can download the config for this model and push it to our repo to see if that resolves the problem. Let‚Äôs try that. Using the techniques we learned in [Chapter 2](https://huggingface.co/course/chapter2/1), we can download the model‚Äôs configuration with the **AutoConfig** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "pretrained_checkpoint = \"distilbert-base-uncased\"\n",
    "config = AutoConfig.from_pretrained(pretrained_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> üö® The approach we‚Äôre taking here is not foolproof, since our colleague may have tweaked the configuration of distilbert-base-uncased before fine-tuning the model. In real life, we‚Äôd want to check with them first, but for the purposes of this section we‚Äôll assume they used the default configuration.\n",
    "\n",
    "We can then push this to our model repository with the configuration‚Äôs **push_to_hub()** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\batuh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\hf_api.py:1001: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28 into local empty directory.\n",
      "Download file pytorch_model.bin:   1%|          | 2.22M/253M [00:02<03:35, 1.22MB/s]\n",
      "Download file pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 252M/253M [03:09<00:00, 1.17MB/s] \n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Download file pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 253M/253M [03:15<00:00, 1.35MB/s]\n",
      "Download file training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.05k/2.05k [03:14<?, ?B/s]\n",
      "\n",
      "\u001b[A\n",
      "Clean file training_args.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.05k/2.05k [03:13<00:00, 5.52B/s]\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\n",
      "\n",
      "Clean file pytorch_model.bin: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 253M/253M [00:05<00:00, 52.1MB/s]\n",
      "To https://huggingface.co/BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28\n",
      "   f708c48..9334476  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/BatuhanYilmaz/distilbert-base-uncased-finetuned-squad-d5716d28/commit/933447601c98ddcc59e5a79fe03d2a8d0e124d89'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config.push_to_hub(model_checkpoint, commit_message=\"Add config.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test if this worked by loading the model from the latest commit on the main branch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.3761070668697357,\n",
       " 'start': 34,\n",
       " 'end': 95,\n",
       " 'answer': 'the task of extracting an answer from a text given a question'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader = pipeline(\"question-answering\", model=model_checkpoint, revision=\"main\")\n",
    "\n",
    "context = r\"\"\"\n",
    "Extractive Question Answering is the task of extracting an answer from a text given a question. An example of a question answering dataset is the SQuAD\n",
    "dataset, which is entirely based on that task. If you would like to fine-tune a\n",
    "model on a SQuAD task, you may leverage the\n",
    "examples/pytorch/question-answering/run_squad.py script.\n",
    "\n",
    "ü§ó Transformers is interoperable with the PyTorch, TensorFlow, and JAX\n",
    "frameworks, so you can use your favourite tools for a wide variety of tasks!\n",
    "\"\"\"\n",
    "\n",
    "question = \"What is extractive question answering?\"\n",
    "reader(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Woohoo, it worked! Let‚Äôs recap what you‚Äôve just learned:\n",
    "- The error messages in Python are known as tracebacks and are read from bottom to top. The last line of the error message usually contains the information you need to locate the source of the problem.\n",
    "- If the last line does not contain sufficient information, work your way up the traceback and see if you can identify where in the source code the error occurs.\n",
    "- If none of the error messages can help you debug the problem, try searching online for a solution to a similar issue.\n",
    "- The huggingface_hub // ü§ó Hub? library provides a suite of tools that you can use to interact with and debug repositories on the Hub.\n",
    "\n",
    "Now that you know how to debug a pipeline, let‚Äôs take a look at a trickier example in the forward pass of the model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging the forward pass of your model\n",
    "Although the pipeline is great for most applications where you need to quickly generate predictions, sometimes you‚Äôll need to access the model‚Äôs logits (say, if you have some custom post-processing that you‚Äôd like to apply). To see what can go wrong in this case, let‚Äôs first grab the model and tokenizer from our pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = reader.tokenizer\n",
    "model = reader.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need a question, so let‚Äôs see if our favorite frameworks are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Which frameworks can I use?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in [Chapter 7](https://huggingface.co/course/chapter7/1), the usual steps we need to take are tokenizing the inputs, extracting the logits of the start and end tokens, and then decoding the answer span:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(question, context, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m      6\u001b[0m answer_start_scores \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mstart_logits\n\u001b[0;32m      7\u001b[0m answer_end_scores \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mend_logits\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:848\u001b[0m, in \u001b[0;36mDistilBertForQuestionAnswering.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;124;03mstart_positions (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;124;03m    Labels for position (index) of the start of the labelled span for computing the token classification loss.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;124;03m    sequence are not taken into account for computing the loss.\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    846\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m--> 848\u001b[0m distilbert_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m distilbert_output[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n\u001b[0;32m    859\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)  \u001b[38;5;66;03m# (bs, max_query_len, dim)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:535\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot specify both input_ids and inputs_embeds at the same time\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 535\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m()\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    537\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m inputs_embeds\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "inputs = tokenizer(question, context, add_special_tokens=True)\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# Get the most likely beginning of answer with the argmax of the score\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# Get the most likely end of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    ")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh dear, it looks like we have a bug in our code! But we‚Äôre not afraid of a little debugging. You can use the Python debugger in a notebook, or in a terminal.\n",
    "\n",
    "Here, reading the error message tells us that 'list' object has no attribute 'size', and we can see a --> arrow pointing to the line where the problem was raised in model(**inputs). <br>\n",
    "You can debug this interactively using the Python debugger, but for now we‚Äôll simply print out a slice of inputs to see what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2029, 7705, 2015, 2064]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This certainly looks like an ordinary Python list, but let‚Äôs double-check the type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep, that‚Äôs a Python list for sure. So what went wrong? Recall from [Chapter 2](https://huggingface.co/course/chapter2/1) that the **AutoModelForXxx classes in ü§ó Transformers operate on tensors (either in PyTorch or TensorFlow), and a common operation is to extract the dimensions of a tensor using Tensor.size()** in, say, PyTorch. Let‚Äôs take another look at the traceback, to see which line triggered the exception:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "~/miniconda3/envs/huggingface/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py in forward(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\n",
    "    471             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "    472         elif input_ids is not None:\n",
    "--> 473             input_shape = input_ids.size()\n",
    "    474         elif inputs_embeds is not None:\n",
    "    475             input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "AttributeError: 'list' object has no attribute 'size'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like our code tried to call input_ids.size(), but this clearly won‚Äôt work for a Python list, which is just a container. How can we solve this problem? <br>\n",
    "Searching for the error message on Stack Overflow gives quite a few [relevant hits](https://stackoverflow.com/search?q=AttributeError%3A+%27list%27+object+has+no+attribute+%27size%27&s=c15ec54c-63cb-481d-a749-408920073e8f&s=8db12e98-b2fe-4618-ad75-63de29ee5359). Clicking on the first one displays a similar question to ours, with the answer shown in the screenshot below:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/stack-overflow.png)\n",
    "\n",
    "The answer recommends that we add return_tensors='pt' to the tokenizer, so let‚Äôs see if that works for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Which frameworks can I use?\n",
      "Answer: pytorch, tensorflow, and jax\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids = inputs[\"input_ids\"][0]\n",
    "outputs = model(**inputs)\n",
    "answer_start_scores = outputs.start_logits\n",
    "answer_end_scores = outputs.end_logits\n",
    "# Get the most likely beginning of answer with the argmax of the score\n",
    "answer_start = torch.argmax(answer_start_scores)\n",
    "# Get the most likely end of answer with the argmax of the score\n",
    "answer_end = torch.argmax(answer_end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end])\n",
    ")\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it worked! This is a great example of how useful Stack Overflow can be: by identifying a similar problem, we were able to benefit from the experience of others in the community. However, a search like this won‚Äôt always yield a relevant answer, so what can you do in such cases? Fortunately, there is a welcoming community of developers on the [Hugging Face forums](https://discuss.huggingface.co) that can help you out! In the next section, we‚Äôll take a look at how you can craft good forum questions that are likely to get answered."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
