{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Asking for help on the forums\n",
    "The [Hugging Face forums](https://discuss.huggingface.co) are a great place to get help from the open source team and wider Hugging Face community. Here’s what the main page looks like on any given day:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forums.png)\n",
    "\n",
    "On the lefthand side you can see all the categories that the various topics are grouped into, while the righthand side shows the most recent topics. A topic is a post that contains a title, category, and description; it’s quite similar to the GitHub issues format that we saw when creating our own dataset in [Chapter 5](https://huggingface.co/course/chapter5/1). As the name suggests, the [Beginners](https://discuss.huggingface.co/c/beginners/5) category is primarily intended for people just starting out with the Hugging Face libraries and ecosystem. Any question on any of the libraries is welcome there, be it to debug some code or to ask for help about how to do something. (That said, if your question concerns one library in particular, you should probably head to the corresponding library category on the forum.)\n",
    "\n",
    "Similarly, the [Intermediate](https://discuss.huggingface.co/c/intermediate/6) and [Research categories](https://discuss.huggingface.co/c/research/7) are for more advanced questions, for example about the libraries or some cool new NLP research that you’d like to discuss.\n",
    "\n",
    "And naturally, we should also mention the [Course](https://discuss.huggingface.co/c/course/20) category, where you can ask any questions you have that are related to the Hugging Face course!\n",
    "\n",
    "Once you have selected a category, you’ll be ready to write your first topic. You can find some [guidelines](https://discuss.huggingface.co/t/how-to-request-support/3128) in the forum on how to do this, and in this section we’ll take a look at some features that make up a good topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a good forum post\n",
    "As a running example, let’s suppose that we’re trying to generate embeddings from Wikipedia articles to create a custom search engine. As usual, we load the tokenizer and model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [03:02<00:00, 1.47MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModel.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose we try to embed a whole section of the [Wikipedia article](https://en.wikipedia.org/wiki/Transformers) on Transformers (the franchise, not the library!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124mGeneration One is a retroactive term for the Transformers characters that\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mappeared between 1984 and 1993. The Transformers began with the 1980s Japanese\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124mCreation Matrix (giving life to Transformers), and its guardian Alpha Trion.\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     43\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m logits \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\u001b[38;5;241m.\u001b[39mlogits\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:550\u001b[0m, in \u001b[0;36mDistilBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    547\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m     inputs_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer(\n\u001b[0;32m    552\u001b[0m     x\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m    553\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    557\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    558\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:133\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    130\u001b[0m word_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embeddings(input_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    131\u001b[0m position_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embeddings(position_ids)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mword_embeddings\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mposition_embeddings\u001b[49m  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    134\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n\u001b[0;32m    135\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(embeddings)  \u001b[38;5;66;03m# (bs, max_seq_length, dim)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (583) must match the size of tensor b (512) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Generation One is a retroactive term for the Transformers characters that\n",
    "appeared between 1984 and 1993. The Transformers began with the 1980s Japanese\n",
    "toy lines Micro Change and Diaclone. They presented robots able to transform\n",
    "into everyday vehicles, electronic items or weapons. Hasbro bought the Micro\n",
    "Change and Diaclone toys, and partnered with Takara. Marvel Comics was hired by\n",
    "Hasbro to create the backstory; editor-in-chief Jim Shooter wrote an overall\n",
    "story, and gave the task of creating the characthers to writer Dennis O'Neil.\n",
    "Unhappy with O'Neil's work (although O'Neil created the name \"Optimus Prime\"),\n",
    "Shooter chose Bob Budiansky to create the characters.\n",
    "\n",
    "The Transformers mecha were largely designed by Shōji Kawamori, the creator of\n",
    "the Japanese mecha anime franchise Macross (which was adapted into the Robotech\n",
    "franchise in North America). Kawamori came up with the idea of transforming\n",
    "mechs while working on the Diaclone and Macross franchises in the early 1980s\n",
    "(such as the VF-1 Valkyrie in Macross and Robotech), with his Diaclone mechs\n",
    "later providing the basis for Transformers.\n",
    "\n",
    "The primary concept of Generation One is that the heroic Optimus Prime, the\n",
    "villainous Megatron, and their finest soldiers crash land on pre-historic Earth\n",
    "in the Ark and the Nemesis before awakening in 1985, Cybertron hurtling through\n",
    "the Neutral zone as an effect of the war. The Marvel comic was originally part\n",
    "of the main Marvel Universe, with appearances from Spider-Man and Nick Fury,\n",
    "plus some cameos, as well as a visit to the Savage Land.\n",
    "\n",
    "The Transformers TV series began around the same time. Produced by Sunbow\n",
    "Productions and Marvel Productions, later Hasbro Productions, from the start it\n",
    "contradicted Budiansky's backstories. The TV series shows the Autobots looking\n",
    "for new energy sources, and crash landing as the Decepticons attack. Marvel\n",
    "interpreted the Autobots as destroying a rogue asteroid approaching Cybertron.\n",
    "Shockwave is loyal to Megatron in the TV series, keeping Cybertron in a\n",
    "stalemate during his absence, but in the comic book he attempts to take command\n",
    "of the Decepticons. The TV series would also differ wildly from the origins\n",
    "Budiansky had created for the Dinobots, the Decepticon turned Autobot Jetfire\n",
    "(known as Skyfire on TV), the Constructicons (who combine to form\n",
    "Devastator),[19][20] and Omega Supreme. The Marvel comic establishes early on\n",
    "that Prime wields the Creation Matrix, which gives life to machines. In the\n",
    "second season, the two-part episode The Key to Vector Sigma introduced the\n",
    "ancient Vector Sigma computer, which served the same original purpose as the\n",
    "Creation Matrix (giving life to Transformers), and its guardian Alpha Trion.\n",
    "\"\"\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh-oh, we’ve hit a problem — and the error message is far more cryptic than the ones we saw in [section 2](https://huggingface.co/course/chapter8/2?fw=pt)! We can’t make head or tails of the full traceback, so we decide to turn to the Hugging Face forums for help. How might we craft the topic?\n",
    "\n",
    "To get started, we need to click the “New Topic” button at the upper-right corner (note that to create a topic, we’ll need to be logged in):\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forums-new-topic.png)\n",
    "\n",
    "This brings up a writing interface where we can input the title of our topic, select a category, and draft the content:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forum-topic01.png)\n",
    "\n",
    "Since the error seems to be exclusively about 🤗 Transformers, we’ll select this for the category. Our first attempt at explaining the problem might look something like this:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forum-topic02.png)\n",
    "\n",
    "Although this topic contains the error message we need help with, there are a few problems with the way it is written:\n",
    "\n",
    "1. The title is not very descriptive, so anyone browsing the forum won’t be able to tell what the topic is about without reading the body as well.\n",
    "2. The body doesn’t provide enough information about where the error is coming from and how to reproduce it.\n",
    "3. The topic tags a few people directly with a somewhat demanding tone.\n",
    "\n",
    "Topics like this one are not likely to get a fast answer (if they get one at all), so let’s look at how we can improve it. We’ll start with the first issue of picking a good title."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing a descriptive title\n",
    "If you’re trying to get help with a bug in your code, a good rule of thumb is to include enough information in the title so that others can quickly determine whether they think they can answer your question or not. In our running example, we know the name of the exception that’s being raised and have some hints that it’s triggered in the forward pass of the model, where we call model(**inputs). To communicate this, one possible title could be:\n",
    "\n",
    "> Source of IndexError in the AutoModel forward pass?\n",
    "\n",
    "This title tells the reader where you think the bug is coming from, and if they’ve encountered an IndexError before, there’s a good chance they’ll know how to debug it. Of course, the title can be anything you want, and other variations like:\n",
    "\n",
    "> Why does my model produce an IndexError?\n",
    "\n",
    "could also be fine. Now that we’ve got a descriptive title, let’s take a look at improving the body."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formatting your code snippets\n",
    "Reading source code is hard enough in an IDE, but it’s even harder when the code is copied and pasted as plain text! Fortunately, the Hugging Face forums support the use of **Markdown**, so you should always enclose your code blocks with three backticks (```) so it’s more easily readable. Let’s do this to prettify the error message — and while we’re at it, let’s make the body a bit more polite than our original version:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forum-topic03.png)\n",
    "\n",
    "As you can see in the screenshot, enclosing the code blocks in backticks converts the raw text into formatted code, complete with color styling! Also note that single backticks can be used to format inline variables, like we’ve done for distilbert-base-uncased. This topic is looking much better, and with a bit of luck we might find someone in the community who can guess what the error is about. However, instead of relying on luck, let’s make life easier by including the traceback in its full gory detail!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including the full traceback\n",
    "Since the last line of the traceback is often enough to debug your own code, it can be tempting to just provide that in your topic to “save space.” Although well intentioned, this actually makes it harder for others to debug the problem since the information that’s higher up in the traceback can be really useful too. So, a good practice is to copy and paste the whole traceback, while making sure that it’s nicely formatted. Since these tracebacks can get rather long, some people prefer to show them after they’ve explained the source code. Let’s do this. Now, our forum topic looks like the following:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forum-topic04.png)\n",
    "\n",
    "This is much more informative, and a careful reader might be able to point out that the problem seems to be due to passing a long input because of this line in the traceback:\n",
    "\n",
    "> Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512).\n",
    "\n",
    "However, we can make things even easier for them by providing the actual code that triggered the error. Let’s do that now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Providing a reproducible example\n",
    "If you’ve ever tried to debug someone else’s code, you’ve probably first tried to recreate the problem they’ve reported so you can start working your way through the traceback to pinpoint the error. It’s no different when it comes to getting (or giving) assistance on the forums, so it really helps if you can provide a small example that reproduces the error. Half the time, simply walking through this exercise will help you figure out what’s going wrong. In any case, the missing piece of our example is to show the inputs that we provided to the model. Doing that gives us something like the following completed example:\n",
    "\n",
    "![](https://huggingface.co/course/static/chapter8/forum-topic05.png)\n",
    "\n",
    "This topic now contains quite a lot of information, and it’s written in a way that is much more likely to attract the attention of the community and get a helpful answer. With these basic guidelines, you can now create great topics to find the answers to your 🤗 Transformers questions!"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "483abfc7fdcd927bfa336910f494643cce94b3dfa36bcded073270b8df64edb3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
