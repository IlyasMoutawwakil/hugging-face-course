{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2v3hBD5XKsM"
      },
      "source": [
        "Install the Transformers and Datasets libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fNINR6i6XKsN",
        "outputId": "16fdec5c-898d-4b39-a21a-89e238b6c488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.17.0-py3-none-any.whl (306 kB)\n",
            "\u001b[K     |████████████████████████████████| 306 kB 31.6 MB/s \n",
            "\u001b[?25hCollecting transformers[sentencepiece]\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 76.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 81.9 MB/s \n",
            "\u001b[?25hCollecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 73.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 71.7 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 73.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Collecting pyyaml\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 69.9 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 57.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 69.5 MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92,>=0.1.91\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 54.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers[sentencepiece]) (1.1.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, pyyaml, asynctest, async-timeout, aiosignal, tokenizers, sacremoses, huggingface-hub, fsspec, aiohttp, xxhash, transformers, sentencepiece, datasets\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.17.0 frozenlist-1.3.0 fsspec-2022.1.0 huggingface-hub-0.4.0 multidict-5.2.0 pyyaml-6.0 sacremoses-0.0.47 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.15.0 xxhash-2.0.2 yarl-1.7.2\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,869 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets transformers[sentencepiece]\n",
        "!apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U46vpaClXKsI"
      },
      "source": [
        "### Sharing pretrained models\n",
        "In the steps below, we’ll take a look at the easiest ways to share pretrained models to the 🤗 Hub. There are tools and utilities available that make it simple to share and update models directly on the Hub, which we will explore below.\n",
        "\n",
        "We encourage all users that train models to contribute by sharing them with the community — sharing models, even when trained on very specific datasets, will help others, saving them time and compute resources and providing access to useful trained artifacts. In turn, you can benefit from the work that others have done!\n",
        "\n",
        "There are three ways to go about creating new model repositories:\n",
        "- Using the **push_to_hub** API\n",
        "- Using the **huggingface_hub** Python library\n",
        "- Using the **web interface**\n",
        "\n",
        "Once you’ve created a repository, you can upload files to it via git and git-lfs. We’ll walk you through creating model repositories and uploading files to them in the following sections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSekBLs8XKsP"
      },
      "source": [
        "You will need to setup git, adapt your email and name in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpOl3bkAXKsQ"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"you@example.com\"\n",
        "!git config --global user.name \"Your Name\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLCN6UD0XKsR"
      },
      "source": [
        "You will also need to be logged in to the Hugging Face Hub. Execute the following and enter your credentials."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWMFN1byj5NM"
      },
      "source": [
        "### Using the push_to_hub API\n",
        "The simplest way to upload files to the Hub is by leveraging the push_to_hub API.\n",
        "\n",
        "Before going further, **you’ll need to generate an authentication token so that the huggingface_hub API knows who you are and what namespaces you have write access to.** Make sure you are in an environment where you have transformers installed. If you are in a notebook, you can use the following function to login:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVmxrEQ6XKsR"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Yw6uWAqkJuQ"
      },
      "source": [
        "In a terminal, you can run: <br>\n",
        "$ huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUJo1OynkQ6J"
      },
      "source": [
        "In both cases, you should be prompted for your username and password, which are the same ones you use to log in to the Hub.\n",
        "\n",
        "Great! You now have your authentication token stored in your cache folder. Let’s create some repositories!\n",
        "\n",
        "If you are using Keras to train your model, the easiest way to upload it to the Hub is to pass along a **PushToHubCallback** when you call **model.fit()**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IY4Gqa8OXKsT"
      },
      "outputs": [],
      "source": [
        "from transformers import PushToHubCallback\n",
        "\n",
        "callback = PushToHubCallback(\n",
        "    \"bert-finetuned-mrpc\", save_strategy=\"epoch\", tokenizer=tokenizer\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHFNQgVbkXnP"
      },
      "source": [
        "Then you should add callbacks=[callback] in your call to model.fit(). The callback will then upload your model to the Hub each time it is saved (here every epoch) in a repository in your namespace. That repository will be named like the output directory you picked (here bert-finetuned-mrpc) but you can choose a different name with hub_model_id = \"a_different_name\".\n",
        "\n",
        "To upload your model to an organization you are a member of, just pass it with hub_model_id = \"my_organization/my_repo_name\".\n",
        "\n",
        "At a lower level, accessing the Model Hub can be done directly on models, tokenizers, and configuration objects via their **push_to_hub()** method. This method takes care of both the repository creation and pushing the model and tokenizer files directly to the repository. No manual handling is required, unlike with the API we’ll see below.\n",
        "\n",
        "To get an idea of how it works, let’s first initialize a model and a tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DlK4rUXSXKsU"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYtF-bvGkbZv"
      },
      "source": [
        "You’re free to do whatever you want with these — add tokens to the tokenizer, train the model, fine-tune it. Once you’re happy with the resulting model, weights, and tokenizer, you can leverage the **push_to_hub()** method directly available on the model object:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEepnHzGXKsV"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(\"dummy-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4KYNqrlk3RR"
      },
      "source": [
        "This will create the new repository dummy-model in your profile, and populate it with your model files. Do the same with the tokenizer, so that all the files are now available in this repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5-fr3iSKXKsV"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siAElExYk8-j"
      },
      "source": [
        "If you belong to an organization, simply specify the organization argument to upload to that organization’s namespace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hH3mp2eBXKsW"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO_gL649k_0u"
      },
      "source": [
        "If you wish to use a specific Hugging Face token, you’re free to specify it to the push_to_hub() method as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2i3Uphe0XKsW"
      },
      "outputs": [],
      "source": [
        "tokenizer.push_to_hub(\"dummy-model\", organization=\"huggingface\", use_auth_token=\"<TOKEN>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9clpsA7IlJxI"
      },
      "source": [
        "Now head to the Model Hub to find your newly uploaded model: https://huggingface.co/user-or-organization/dummy-model.\n",
        "\n",
        "Click on the “Files and versions” tab, and you should see the files visible in the following screenshot:\n",
        "![](https://huggingface.co/course/static/chapter4/push_to_hub_dummy_model_tf.png)\n",
        "\n",
        "As you’ve seen, the push_to_hub() method accepts several arguments, making it possible to upload to a specific repository or organization namespace, or to use a different API token. We recommend you take a look at the method specification available directly in the 🤗 [Transformers documentation](https://huggingface.co/docs/transformers/model_sharing) to get an idea of what is possible.\n",
        "\n",
        "The **push_to_hub()** method is backed by the huggingface_hub Python package, which offers a direct API to the Hugging Face Hub. It’s integrated within 🤗 Transformers and several other machine learning libraries, like allenlp. Although we focus on the 🤗 Transformers integration in this chapter, integrating it into your own code or library is simple.\n",
        "\n",
        "Jump to the last section to see how to upload files to your newly created repository!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ywrc2l7zls0B"
      },
      "source": [
        "### Using the huggingface_hub Python library\n",
        "The huggingface_hub Python library is a package which offers a set of tools for the model and datasets hubs. It provides simple methods and classes for common tasks like getting information about repositories on the hub and managing them. It provides simple APIs that work on top of git to manage those repositories’ content and to integrate the Hub in your projects and libraries.\n",
        "\n",
        "Similarly to using the push_to_hub API, this will require you to have your API token saved in your cache. In order to do this, you will need to use the login command from the CLI, as mentioned in the previous section (again, make sure to prepend these commands with the ! character if running in Google Colab):\n",
        "\n",
        "$ huggingface-cli login\n",
        "\n",
        "The huggingface_hub package offers several methods and classes which are useful for our purpose. Firstly, there are a few methods to manage repository creation, deletion, and others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APBqY0GzXKsX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import (\n",
        "    # User management\n",
        "    login,\n",
        "    logout,\n",
        "    whoami,\n",
        "\n",
        "    # Repository creation and management\n",
        "    create_repo,\n",
        "    delete_repo,\n",
        "    update_repo_visibility,\n",
        "\n",
        "    # And some methods to retrieve/change information about the content\n",
        "    list_models,\n",
        "    list_datasets,\n",
        "    list_metrics,\n",
        "    list_repo_files,\n",
        "    upload_file,\n",
        "    delete_file,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af-IzPaymJS8"
      },
      "source": [
        "Additionally, it offers the very powerful **Repository class** to manage a local repository. We will explore these methods and that class in the next few section to understand how to leverage them.\n",
        "\n",
        "The **create_repo** method can be used to create a new repository on the hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyywwIIbXKsX"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AP9w_vdImT-3"
      },
      "source": [
        "This will create the repository dummy-model in your namespace. If you like, you can specify which organization the repository should belong to using the organization argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ViGpB8BLXKsY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"dummy-model\", organisation=\"huggingface\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMnkHXvQm5ua"
      },
      "source": [
        "This will create the dummy-model repository in the huggingface namespace, assuming you belong to that organization. Other arguments which may be useful are:\n",
        "\n",
        "- private, in order to specify if the repository should be visible from others or not.\n",
        "- token, if you would like to override the token stored in your cache by a given token.\n",
        "- repo_type, if you would like to create a dataset or a space instead of a model. Accepted values are \"dataset\" and \"space\".\n",
        "\n",
        "Once the repository is created, we should add files to it! Jump to the next section to see the three ways this can be handled."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TC-WQBGHnM1i"
      },
      "source": [
        "### Using the web interface\n",
        "The web interface offers tools to manage repositories directly in the Hub. Using the interface, you can easily create repositories, add files (even large ones!), explore models, visualize diffs, and much more.\n",
        "\n",
        "To create a new repository, visit [huggingface.co/new](https://huggingface.co/new):\n",
        "![](https://huggingface.co/course/static/chapter4/new_model.png)\n",
        "\n",
        "First, specify the owner of the repository: this can be either you or any of the organizations you’re affiliated with. If you choose an organization, the model will be featured on the organization’s page and every member of the organization will have the ability to contribute to the repository.\n",
        "\n",
        "Next, enter your model’s name. This will also be the name of the repository. Finally, you can specify whether you want your model to be public or private. Private models require you to have a paid Hugging Face account, and will allow you have models hidden from public view.\n",
        "\n",
        "After creating your model repository, you should see a page like this:\n",
        "![](https://huggingface.co/course/static/chapter4/empty_model.png)\n",
        "\n",
        "This is where your model will be hosted. To start populating it, you can add a README file directly from the web interface.\n",
        "\n",
        "The README file is in Markdown — feel free to go wild with it! The third part of this chapter is dedicated to building a model card. These are of prime importance in bringing value to your model, as they’re where you tell others what it can do.\n",
        "\n",
        "If you look at the “Files and versions” tab, you’ll see that there aren’t many files there yet — just the README.md you just created and the .gitattributes file that keeps track of large files.\n",
        "\n",
        "We’ll take a look at how to add some new files next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrRI1UFdoCqc"
      },
      "source": [
        "### Uploading the model files\n",
        "The system to manage files on the Hugging Face Hub is based on git for regular files, and git-lfs (which stands for Git Large File Storage) for larger files.\n",
        "\n",
        "In the next section, we go over three different ways of uploading files to the Hub: through huggingface_hub and through git commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbNLcWHYoSyc"
      },
      "source": [
        "### The upload_file approach\n",
        "Using **upload_file** does not require git and git-lfs to be installed on your system. It pushes files directly to the 🤗 Hub using HTTP POST requests. A limitation of this approach is that it doesn’t handle files that are larger than 5GB in size. If your files are larger than 5GB, please follow the two other methods detailed below.\n",
        "\n",
        "The API may be used as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49aatQguXKsY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "upload_file(\n",
        "    \"<path_to_file>/config.json\",\n",
        "    path_in_repo=\"config.json\",\n",
        "    repo_id=\"<namespace>/dummy-model\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ssGkIC81odYY"
      },
      "source": [
        "This will upload the file config.json available at <path_to_file> to the root of the repository as config.json, to the dummy-model repository. Other arguments which may be useful are:\n",
        "\n",
        "- **token**, if you would like to override the token stored in your cache by a given token.\n",
        "- **repo_type**, if you would like to upload to a dataset or a space instead of a model. Accepted values are \"dataset\" and \"space\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYMbWQFXonit"
      },
      "source": [
        "### The Repository class\n",
        "**The Repository** class manages a local repository in a git-like manner. It abstracts most of the pain points one may have with git to provide all features that we require.\n",
        "\n",
        "Using this class requires having git and git-lfs installed, so make sure you have git-lfs installed (see [here](https://git-lfs.github.com) for installation instructions) and set up before you begin.\n",
        "\n",
        "In order to start playing around with the repository we have just created, we can start by initialising it into a local folder by cloning the remote repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51QxciYZXKsY"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import Repository\n",
        "\n",
        "repo = Repository(\"<path_to_dummy_folder>\", clone_from=\"<namespace>/dummy-model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShTT_wo6o-Lg"
      },
      "source": [
        "This created the folder path_to_dummy_folder in our working directory. This folder only contains the .gitattributes file as that’s the only file created when instantiating the repository through create_repo.\n",
        "\n",
        "From this point on, we may leverage several of the traditional git methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_chikSyRXKsZ"
      },
      "outputs": [],
      "source": [
        "repo.git_pull()\n",
        "repo.git_add()\n",
        "repo.git_commit()\n",
        "repo.git_push()\n",
        "repo.git_tag()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2OuugO4pOHI"
      },
      "source": [
        "And others! We recommend taking a look at the Repository documentation available [here](https://github.com/huggingface/huggingface_hub/tree/main/src/huggingface_hub#advanced-programmatic-repository-management) for an overview of all available methods.\n",
        "\n",
        "At present, we have a model and a tokenizer that we would like to push to the hub. We have successfully cloned the repository, we can therefore save the files within that repository.\n",
        "\n",
        "We first make sure that our local clone is up to date by pulling the latest changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btROUCkYXKsZ"
      },
      "outputs": [],
      "source": [
        "repo.git_pull()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWHUpg8upZzM"
      },
      "source": [
        "Once that is done, we save the model and tokenizer files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3dlgMPVXKsZ"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dRGzirxbpc87"
      },
      "source": [
        "The path_to_dummy_folder now contains all the model and tokenizer files. We follow the usual git workflow by adding files to the staging area, committing them and pushing them to the hub:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MGvRKElXKsa"
      },
      "outputs": [],
      "source": [
        "repo.git_add()\n",
        "repo.git_commit(\"Add model and tokenizer files\")\n",
        "repo.git_push()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mPEYBtWpkiZ"
      },
      "source": [
        "Congratulations! You just pushed your first files on the hub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LkRma7xmpoYJ"
      },
      "source": [
        "Let's generate a model and tokenizer that we’d like to commit to our dummy repository:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNelpsciXKsa"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForMaskedLM, AutoTokenizer\n",
        "\n",
        "checkpoint = \"camembert-base\"\n",
        "\n",
        "model = TFAutoModelForMaskedLM.from_pretrained(checkpoint)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "# Do whatever with the model, train it, fine-tune it...\n",
        "\n",
        "model.save_pretrained(\"<path_to_dummy_folder>\")\n",
        "tokenizer.save_pretrained(\"<path_to_dummy_folder>\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvWrttzxqGRF"
      },
      "source": [
        "Now  we’ve saved some model and tokenizer artifacts. We can now go ahead and proceed like we would usually do with traditional Git repositories. We can add all the files to Git’s staging environment using the git add command:\n",
        "\n",
        "$ git add .\n",
        "\n",
        "We can then have a look at the files that are currently staged:\n",
        "\n",
        "$ git status\n",
        "\n",
        "Similarly, we can make sure that git-lfs is tracking the correct files by using its status command:\n",
        "\n",
        "$ git lfs status\n",
        "\n",
        "We can see that all files have Git as a handler, except t5_model.h5, which has LFS. Great!\n",
        "\n",
        "Let’s proceed to the final steps, committing and pushing to the huggingface.co remote repository:\n",
        "\n",
        "$ git commit -m \"First model version\"\n",
        "\n",
        "Pushing can take a bit of time, depending on the speed of your internet connection and the size of your files:\n",
        "\n",
        "$ git push\n",
        "\n",
        "If we take a look at the model repository when this is finished, we can see all the recently added files."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Copy of Sharing pretrained models (TensorFlow)",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
